{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49ac566",
   "metadata": {},
   "source": [
    "## Calculate confusion matrices and performance metrics using pretrained weights, domain adaptation with ensemble classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c61eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ecg_utilities import *\n",
    "from os.path import join as osj\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as Func\n",
    "\n",
    "from pytorch_sklearn import NeuralNetwork\n",
    "from pytorch_sklearn.callbacks import WeightCheckpoint, Verbose, LossPlot, EarlyStopping, Callback, CallbackInfo\n",
    "from pytorch_sklearn.utils.func_utils import to_safe_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13db535",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = pd.read_csv(osj(\"..\", \"files\", \"patient_ids.csv\"), header=None).to_numpy().reshape(-1)\n",
    "valid_patients = pd.read_csv(osj(\"..\", \"files\", \"valid_patients.csv\"), header=None).to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ef5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# privacy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = osj(\"..\", \"data_single\", \"dataset_training\", \"domain_adapted\")\n",
    "TRIO_PATH = osj(\"..\", \"data_trio\", \"dataset_training\", \"domain_adapted\")\n",
    "\n",
    "# if privacy:\n",
    "#     DATASET_PATH = osj(DATASET_PATH,  \"domain_adapted_dp_ln\")\n",
    "#     TRIO_PATH = osj(TRIO_PATH,  \"domain_adapted_dp_ln\")\n",
    "#     LOAD_PATH = osj(\"..\", \"savefolder_dp\", \"nets_ln_30\") # self-trained dp\n",
    "#     # DATASET_PATH = osj(DATASET_PATH,  \"domain_adapted_dp_ld\")\n",
    "#     # TRIO_PATH = osj(TRIO_PATH,  \"domain_adapted_dp_ld\")\n",
    "#     # LOAD_PATH = osj(\"..\", \"savefolder_dp\", \"nets_ld\") # self-trained dp\n",
    "#     # DATASET_PATH = osj(DATASET_PATH,  \"domain_adapted_dp_l\")\n",
    "#     # TRIO_PATH = osj(TRIO_PATH,  \"domain_adapted_dp_l\")\n",
    "#     # LOAD_PATH = osj(\"..\", \"savefolder_dp\", \"nets_l\") # self-trained dp\n",
    "#     DICT_PATH = osj(\"..\", \"dictionaries\", \"5min_sorted_dp\")\n",
    "#     SAVE_PATH = osj(\"..\", \"savefolder_dp\", \"ens\")\n",
    "    \n",
    "# else:\n",
    "DICT_PATH = osj(\"..\", \"data_single\", \"dictionaries\", \"5min_sorted\")\n",
    "\n",
    "SAVE_PATH = osj(\"..\", \"savefolder\", \"DA_Ens\")\n",
    "\n",
    "#LOAD_PATH = osj(\"..\", \"pretrained\", \"nets\") # pretrained\n",
    "LOAD_PATH = osj(\"..\", \"savefolder\", \"nets\") # self-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b8f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = [-1]\n",
    "batch_sizes = [1024]\n",
    "confidences = [0, *np.linspace(0.5, 1, 51)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4bc092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_array_of_dict_of_confmat(arr):\n",
    "    return np.stack([np.stack(list(d.values())) for d in arr])\n",
    "\n",
    "def extract_array_of_dict_of_dict_of_confmat(arr):\n",
    "    return np.stack([np.stack([np.stack(list(d2.values())) for d2 in d1.values()]) for d1 in arr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc8718",
   "metadata": {},
   "source": [
    "What is collected?\n",
    "- Per repeat:\n",
    "    - Confusion matrices per patient (34 in total).\n",
    "    - Cumulative confusion matrix (1 in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c97ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "all_patient_cms = []\n",
    "all_cms = []\n",
    "all_confs = []\n",
    "repeats = 10\n",
    "\n",
    "for repeat in range(repeats):\n",
    "    patient_cms = {conf:{} for conf in confidences}\n",
    "    cm = {conf:torch.zeros(2, 2) for conf in confidences}\n",
    "    confs = []\n",
    "    \n",
    "    for i, patient_id in enumerate(valid_patients):\n",
    "        dataset = load_N_channel_dataset(patient_id, DATASET_PATH, TRIO_PATH)\n",
    "        train_X, train_y, train_ids, val_X, val_y, val_ids, test_X, test_y, test_ids = dataset.values()\n",
    "        \n",
    "        # For consulting through error energy.\n",
    "        D, F = load_dictionary(patient_id, DICT_PATH)\n",
    "        D, F = torch.Tensor(D), torch.Tensor(F)\n",
    "\n",
    "        ## Consulting Exponential - Gaussian.\n",
    "        BF = BayesianFit()\n",
    "        EF = ExponentialFit()\n",
    "        GF = GaussianFit()\n",
    "\n",
    "        # Train error.\n",
    "        train_E, E_healthy, E_arrhyth = get_error_one_patient(train_X[:, 0, :].squeeze(), F, y=train_y, as_energy=True)\n",
    "        # _, E_healthy, E_arrhyth = get_error_per_patient(train_X[:, 0, :].squeeze(), ids=train_ids, DICT_PATH=DICT_PATH, y=train_y, as_energy=True)\n",
    "        \n",
    "        EF.fit(E_healthy)\n",
    "        GF.fit(E_arrhyth)\n",
    "        consult_train_y = torch.Tensor(BF.predict(train_E, EF, GF) <= 0.5).long()\n",
    "        \n",
    "        # Test Error (be careful, we check (<= 0.5) because EF => healthy => label 0)\n",
    "        test_E = get_error_one_patient(test_X[:, 0, :].squeeze(), F, as_energy=True)\n",
    "        \n",
    "        EF.fit(E_healthy)\n",
    "        GF.fit(E_arrhyth)\n",
    "        consult_test_y = torch.Tensor(BF.predict(test_E, EF, GF) <= 0.5).long()\n",
    "\n",
    "        # Load the neural network.\n",
    "        model = get_base_model(in_channels=train_X.shape[1])\n",
    "        model = model.to(\"cuda\")\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        optim = torch.optim.AdamW(params=model.parameters())\n",
    "        \n",
    "        net = NeuralNetwork.load_class(osj(LOAD_PATH, f\"net_{repeat+1}_{patient_id}\"), model, optim, crit)\n",
    "        weight_checkpoint_val_loss = net.cbmanager.callbacks[1]  # <- this needs to change in case weight checkpoint is not the second callback.\n",
    "        \n",
    "        net.load_weights(weight_checkpoint_val_loss)\n",
    "        \n",
    "        # Test predictions and probabilities.\n",
    "        pred_y = net.predict(test_X, batch_size=1024, use_cuda=True, fits_gpu=True, decision_func=lambda pred_y: pred_y.argmax(dim=1)).cpu()\n",
    "        prob_y = net.predict_proba(test_X).cpu()\n",
    "        softmax_prob_y = Func.softmax(prob_y, dim=1).max(dim=1).values\n",
    "        \n",
    "        for conf in confidences:\n",
    "            low_confidence = softmax_prob_y < conf\n",
    "            high_confidence = softmax_prob_y >= conf\n",
    "\n",
    "            final_pred_y = torch.Tensor(np.select([low_confidence, high_confidence], [consult_test_y, pred_y])).long()\n",
    "            cm_exp = get_confusion_matrix(final_pred_y, test_y, pos_is_zero=False)\n",
    "\n",
    "            patient_cms[conf][patient_id] = cm_exp\n",
    "            cm[conf] += cm_exp\n",
    "            \n",
    "        print_progress(i + 1, len(valid_patients), opt=[f\"{patient_id}\"])\n",
    "        \n",
    "    all_patient_cms.append(patient_cms)\n",
    "    all_cms.append(cm)\n",
    "    all_confs.append(confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e735d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=max_epochs[0],\n",
    "    batch_size=batch_sizes[0],\n",
    "    optimizer=optim.__class__.__name__,\n",
    "    loss=crit.__class__.__name__,\n",
    "    early_stopping=\"true\",\n",
    "    checkpoint_on=weight_checkpoint_val_loss.tracked,\n",
    "    dataset=\"default+trio\",\n",
    "    info=\"Results replicated for GitHub, DA + Ensemble + All C.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0b0f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cms = extract_array_of_dict_of_confmat(all_cms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f16d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9752713888586915,\n",
       " 'rec': 0.8854954163003893,\n",
       " 'spe': 0.9879403842064224,\n",
       " 'pre': 0.9119857211221336,\n",
       " 'npv': 0.983907319716269,\n",
       " 'f1': 0.8985453688187732}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_performance_metrics(all_cms.sum(axis=0)[0])  # only DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4ca5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9467860415275426,\n",
       " 'rec': 0.7952530453346729,\n",
       " 'spe': 0.968170057418303,\n",
       " 'pre': 0.7790421592628588,\n",
       " 'npv': 0.9710214477545355,\n",
       " 'f1': 0.7870641386553318}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_performance_metrics(all_cms.sum(axis=0)[-1]) # only NPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd69a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9785720431202984,\n",
       " 'rec': 0.905727489639583,\n",
       " 'spe': 0.9888517087787237,\n",
       " 'pre': 0.9197748764758874,\n",
       " 'npv': 0.9867250781242871,\n",
       " 'f1': 0.9126971351746451}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_performance_metrics(all_cms[:, 1:-1, :, :].sum(axis=(0, 1))) # avg over quantized C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bad338",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_performance = dict.fromkeys([\"DA\", \"NPE\", \"ENS\"])\n",
    "all_performance[\"DA\"]  = get_performance_metrics(all_cms.sum(axis=0)[0])  # only DA\n",
    "all_performance[\"NPE\"] = get_performance_metrics(all_cms.sum(axis=0)[-1]) # only NPE\n",
    "all_performance[\"ENS\"] = get_performance_metrics(all_cms[:, 1:-1, :, :].sum(axis=(0, 1))) # avg over quantized C\n",
    "\n",
    "if False:\n",
    "    with open(osj(SAVE_PATH, \"cms.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(all_cms, f)\n",
    "        \n",
    "    with open(osj(SAVE_PATH, \"config.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(config, f)\n",
    "        \n",
    "    with open(osj(SAVE_PATH, \"patient_cms.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(all_patient_cms, f)\n",
    "        \n",
    "    with open(osj(SAVE_PATH, \"confidences.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(all_confs, f)\n",
    "\n",
    "    with open(osj(SAVE_PATH, \"performances.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(all_performance, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc9fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DA': {'acc': 0.9752713888586915,\n",
       "  'rec': 0.8854954163003893,\n",
       "  'spe': 0.9879403842064224,\n",
       "  'pre': 0.9119857211221336,\n",
       "  'npv': 0.983907319716269,\n",
       "  'f1': 0.8985453688187732},\n",
       " 'NPE': {'acc': 0.9467860415275426,\n",
       "  'rec': 0.7952530453346729,\n",
       "  'spe': 0.968170057418303,\n",
       "  'pre': 0.7790421592628588,\n",
       "  'npv': 0.9710214477545355,\n",
       "  'f1': 0.7870641386553318},\n",
       " 'ENS': {'acc': 0.9785720431202984,\n",
       "  'rec': 0.905727489639583,\n",
       "  'spe': 0.9888517087787237,\n",
       "  'pre': 0.9197748764758874,\n",
       "  'npv': 0.9867250781242871,\n",
       "  'f1': 0.9126971351746451}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dpl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
